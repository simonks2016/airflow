[core]
# The home directory for Airflow
airflow_home = /opt/airflow
# The default timezone to use for the scheduler and the webserver
default_timezone = UTC
# The SQLAlchemy connection string for the metadata database
sql_alchemy_conn = db+postgresql://airflow:airflow@postgres/airflow
# Airflow executor (SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor)
executor = LocalExecutor
# The folder where your airflow logs are stored
base_log_folder = /opt/airflow/logs

# Webserver settings
[webserver]
web_server_host = 0.0.0.0
web_server_port = 8080
# The number of Gunicorn workers for the web server
workers = 4
# The worker class for Gunicorn
worker_class = sync
# Web server log level, choices are debug, info, warn, error, critical
web_server_log_level = INFO
# The directory where the web server will look for static files
web_server_worker_timeout = 120

# Scheduler settings
[scheduler]
scheduler_queued_task_running_timeout = 600
scheduler_task_queued_timeout = 600
# The time delay for scheduling new DAGs
scheduler_run_duration = -1
# The number of times that scheduler should retry if a task fails
scheduler_task_queued_retry_delay = 5

[logging]
# Whether to log to syslog or not (default: False)
log_to_syslog = False

# The logging level for Airflow logs
log_level = INFO

# The handler used to log to a file (if False, logs will not be written to file)
file_task_handler = True

# The folder where logs are stored
# Logs are stored in a separate subfolder for each task
task_log_prefix_template = "{{ task_instance.dag_id }}/{{ task_instance.task_id }}"

[database]
# Database conn
sql_alchemy_conn = db+postgresql://airflow:airflow@postgres/airflow
# Whether to autoincrement the id columns (default: True)
sql_alchemy_autoincrement = True
# Whether to use pools
use_pool = True
